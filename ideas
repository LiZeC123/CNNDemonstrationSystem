
反向传播的理解
---------------
函数之间的关系决定了前一级的输入是应该变大还是变小。
例如全连接层，
输出神经元希望变大，则正权值连接的神经元需要变大，负权值连接的神经元需要变小
（实际上就是取决于此处的梯度表达式（应该是权值大的变化大，权值小的变化小））

所有输出神经元对一个输入神经元的期望之和就是这个神经元变化的期望，
从而可以指导前一级神经元的变化

注意区分神经元值期望的变化和偏置期望的变化


计算梯度时，前一层的梯度值需要当前层的梯度值，当前层对神经元的偏导数是梯度值的一部分
因此计算梯度时，需要依赖前一层的计算结果，一步一步得到最后的计算结果


为什么这样设计卷积核的数量
----------------------------
卷积和池化过程中的神经元具有一定的空间结构，因此需要尽量展现全貌
由于空间的限制，卷积核的数量不能太多

全连接层的特点
-----------------
进入全连接层后，神经元之间不再具有空间结构，因此可以使用滚动的方式全部神经元
并且加上数字来展示计算和反向传播的细节

全连接层的数据也需要经过ReLU处理，因此在界面上只显示经过处理的结果
因此神经元本身不存在负值
